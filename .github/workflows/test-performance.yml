# For more information see:
# https://pytest-benchmark.readthedocs.io

name: Test performance

on:
  push:
    branches: ["main"]
    paths:
      - benchmarks/**
      - .github/workflows/test-performance.yml
  workflow_dispatch:

permissions:
  contents: read

jobs:
  test:
    name: indicators
    runs-on: ubuntu-latest

    steps:
      - name: Checkout source
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 9.x
          dotnet-quality: ga

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.12
          cache: "pip"

      - name: Setup GitVersion
        uses: gittools/actions/gitversion/setup@v3.1.1
        with:
          versionSpec: 6.0.5

      - name: Execute GitVersion
        id: gitversion
        uses: gittools/actions/gitversion/execute@v3.1.1

      - name: Install dependencies
        run: |
          pip install -U --upgrade-strategy=only-if-needed pip
          pip install -r requirements.txt
          pip install -r requirements-test.txt

      - name: Test performance
        run: >
          pytest benchmarks
          --benchmark-json=benchmark_data.json
          --benchmark-sort=name
          -m "performance"

      - name: Publish test results
        run: |
          # Collect system info
          echo "### Performance test results" > report.md
          echo "" >> report.md
          echo "**Package version:** ${{ steps.gitversion.outputs.fullSemVer }}" >> report.md
          echo "" >> report.md
          echo "**Test environment:**" >> report.md
          echo "- OS: $(uname -s) $(uname -r)" >> report.md
          echo "- CPU: $(cat /proc/cpuinfo | grep 'model name' | head -n1 | cut -d':' -f2 | xargs)" >> report.md
          echo "- Memory: $(free -h | awk '/^Mem:/ {print $2}')" >> report.md
          echo "" >> report.md

          # Convert benchmark data to table
          python -c '
          import json
          with open("benchmark_data.json") as f:
              data = json.load(f)

          print("| Indicator | Mean (ms) | StdDev (ms) | Ops/sec |", file=open("report.md", "a"))
          print("| :-------- | --------: | ----------: | ------: |", file=open("report.md", "a"))

          for bm in sorted(data["benchmarks"], key=lambda x: x["fullname"]):
              # Extract just the indicator name from the test name
              name = bm["fullname"].split("test_benchmark_")[-1]
              name = name.split("::")[-1].upper()

              # Convert seconds to milliseconds
              mean_ms = bm["stats"]["mean"] * 1000
              stddev_ms = bm["stats"]["stddev"] * 1000
              ops = bm["stats"]["ops"]

              print(f"| {name} | {mean_ms:.3f} | {stddev_ms:.3f} | {ops:.2f} |",
                    file=open("report.md", "a"))
          '

      - name: Publish benchmark results
        run: cat report.md >> $GITHUB_STEP_SUMMARY
